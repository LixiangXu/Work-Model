{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "Strunturally constrained recurrent network(SCRN)\n",
    "Goal is to predict the next token in the sequence given its past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import package\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time as time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data import parameter\n",
    "#filename_input = \"words.txt\"\n",
    "#filename_input = \"words_total.txt\"\n",
    "filename_input = \"words_16.txt\"\n",
    "#filename_input = \"words_21.txt\"\n",
    "#filename_input = \"words_26.txt\"\n",
    "#filename_input = \"words_42.txt\"\n",
    "eval_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NN parameter\n",
    "input_size = 26 # one-hot input of characters\n",
    "s_size = 10 # size of context layer(slow)\n",
    "h_size = 40 # size of hidden layer(fast)\n",
    "output_size = 26 # output size, one-hot output of character\n",
    "alpha = 0.95 # 0.95\n",
    "num_fw = 5 # forward steps\n",
    "num_bp = 50 # 50 BPTT steps\n",
    "lr = 2.5 # initial learning rate\n",
    "batch_size = 32\n",
    "Epochs = 6000 # number of epochs\n",
    "per_epoch = 100 # epochs to display loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import file: words_16.txt with Number of chars: 172491\n"
     ]
    }
   ],
   "source": [
    "# import data from dataset, articles or reviews\n",
    "with open(filename_input) as file_input:\n",
    "    data_str = file_input.read()\n",
    "print('Import file: '+filename_input+' with Number of chars: ' + str(len(data_str)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = int((len(data_str)-num_fw)/num_bp/batch_size)\n",
    "data_mat = np.asarray([ord(char)-ord('a') for char in data_str])\n",
    "\n",
    "train_data_len = int((steps-eval_steps)*num_bp*batch_size)\n",
    "eval_data_len = int(eval_steps*num_bp*batch_size)\n",
    "\n",
    "train_data_mat = np.zeros([train_data_len, num_fw])\n",
    "train_data_y = np.zeros([train_data_len])\n",
    "eval_data_x = np.zeros([eval_data_len, num_fw])\n",
    "eval_data_y = np.zeros([eval_data_len])\n",
    "train_data_x = list()\n",
    "train_data_y = list()\n",
    "eval_data_x = list()\n",
    "eval_data_y = list()\n",
    "\n",
    "for ii in range(train_data_len):\n",
    "    train_data_x.append(data_mat[ii:ii+num_fw])\n",
    "    train_data_y.append(data_mat[ii+num_fw])\n",
    "for ii in range(eval_data_len):\n",
    "    jj = ii + train_data_len\n",
    "    eval_data_x.append(data_mat[jj:jj+num_fw])\n",
    "    eval_data_y.append(data_mat[jj+num_fw])\n",
    "\n",
    "train_data_batchx = list()\n",
    "train_data_batchy = list()\n",
    "per_batch = int((steps-eval_steps)*num_bp)\n",
    "for i in range(batch_size):\n",
    "    train_data_batchx.append(train_data_x[i*per_batch: (i+1)*per_batch])\n",
    "    train_data_batchy.append(train_data_y[i*per_batch: (i+1)*per_batch])\n",
    "\n",
    "train_data_batchx = np.asarray(train_data_batchx)\n",
    "train_data_batchy = np.asarray(train_data_batchy)\n",
    "train_data_stepx = list()\n",
    "train_data_stepy = list()\n",
    "for i in range(int(steps-eval_steps)):\n",
    "    train_data_stepx.append(train_data_batchx[:, i*num_bp:(i+1)*num_bp, :])\n",
    "    train_data_stepy.append(train_data_batchy[:, i*num_bp:(i+1)*num_bp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 105 steps, 32 batches, 50 bps, 5 fws\n",
      "Eval data size: 3200\n"
     ]
    }
   ],
   "source": [
    "print('Train data size: %d steps, %d batches, %d bps, %d fws'%(len(train_data_stepx), batch_size, num_bp, num_fw))\n",
    "print('Eval data size: %d'%(len(eval_data_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define structure of SCRN\n",
    "trainX = list()\n",
    "trainY = list()\n",
    "for i in range(num_fw + num_bp):\n",
    "    trainX.append(tf.placeholder(tf.float32, shape=(batch_size, input_size)))\n",
    "for i in range(num_bp):\n",
    "    trainY.append(tf.placeholder(tf.float32, shape=(batch_size, output_size)))\n",
    "    \n",
    "s_first = tf.placeholder(tf.float32, shape=(batch_size, s_size), name='1st_s')\n",
    "h_first = tf.placeholder(tf.float32, shape=(batch_size, h_size), name='1st_h')\n",
    "\n",
    "diag_I = np.ones([s_size]).astype('float32')\n",
    "diag_Q = alpha*np.ones([s_size]).astype('float32')\n",
    "I = tf.diag(diag_I, name=\"I\")\n",
    "Q = tf.diag(diag_Q, name=\"Q\")\n",
    "B = tf.Variable(tf.truncated_normal([input_size, s_size], -0.1,0.1), name='B')\n",
    "\n",
    "P = tf.Variable(tf.truncated_normal([s_size, h_size], -0.1,0.1), name='P')\n",
    "A = tf.Variable(tf.truncated_normal([input_size, h_size], -0.1,0.1), name='A')\n",
    "R = tf.Variable(tf.truncated_normal([h_size, h_size], -0.1,0.1), name='R')\n",
    "\n",
    "U = tf.Variable(tf.truncated_normal([h_size, output_size], -0.1,0.1), name='U')\n",
    "V = tf.Variable(tf.truncated_normal([s_size, output_size], -0.1,0.1), name='V')\n",
    "\n",
    "\n",
    "def SCRN(x, s_pre, h_pre):\n",
    "    s_after = tf.matmul(x, tf.matmul(B, (I-Q))) + tf.matmul(s_pre, Q) # s_t = (I-Q)Bx_t + Qs_t-1\n",
    "    h_after = tf.sigmoid(tf.matmul(s_after, P)+tf.matmul(x, A)+tf.matmul(h_pre,R)) #h_t = sigmoid(Ps_t+Ax_t+Rh_t-1)\n",
    "    y_after = tf.nn.softmax(tf.matmul(h_after, U)+tf.matmul(s_after, V)) #y_t = f(Uh_t + Vs_t)\n",
    "    \n",
    "    return s_after, h_after, y_after\n",
    "\n",
    "\n",
    "# forward\n",
    "for i in range(num_bp):\n",
    "    j = 0\n",
    "    if i == 0:\n",
    "        outputs = list()\n",
    "        s_pre = s_first\n",
    "        h_pre = h_first\n",
    "    \n",
    "    for j in range(num_fw):\n",
    "        if j<num_fw-1:\n",
    "            s_after, h_after, _ = SCRN(trainX[i+j], s_pre, h_pre)\n",
    "        else:\n",
    "            s_after, h_after,y_after = SCRN(trainX[i+j], s_pre, h_pre)\n",
    "        \n",
    "        \"\"\"        \n",
    "        #dropout\n",
    "        s_after = tf.nn.dropout(s_after, keep_prob=0.8)\n",
    "        h_after = tf.nn.dropout(h_after, keep_prob=0.9)\n",
    "        \"\"\"       \n",
    "        \n",
    "        s_pre = s_after\n",
    "        h_pre = h_after\n",
    "    \n",
    "    \n",
    "    outputs.append(y_after)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train opt\n",
    "\n",
    "#log likelihood loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.concat(outputs,0),labels=tf.concat(trainY,0)))\n",
    "#loss = tf.losses.mean_squared_error(labels=tf.concat(trainY,0), predictions = tf.concat(outputs,0))\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "#optimizer\n",
    "global_epoch = tf.Variable(0)\n",
    "#learning_rate = tf.train.exponential_decay(\n",
    "#    learning_rate=, global_step=global_epoch, decay_steps=5000, decay_rate=0.9, staircase=True)\n",
    "#learning_rate = lr\n",
    "\n",
    "#optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.5, use_nesterov=True)\n",
    "\n",
    "\n",
    "gradients,var=zip(*optimizer.compute_gradients(loss))\n",
    "gradients_clipped, _ = tf.clip_by_global_norm(gradients, 0.1)\n",
    "opt=optimizer.apply_gradients(zip(gradients_clipped,var),global_step=global_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize all Variables\n"
     ]
    }
   ],
   "source": [
    "#add init op to the graph\n",
    "train_filename = \"SCRN_hsize_\"+str(h_size)+\"_ssize_\"+str(s_size)+\"_fw_\"+str(num_fw)+\"_bp_\"+str(num_bp)+\".ckpt\"\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "cwd = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(train_filename+\".index\"):\n",
    "    #load Vars\n",
    "    saver.restore(sess, train_filename)\n",
    "    print(\"Model restored from: %s\" %(train_filename))\n",
    "else:\n",
    "    #init Vars\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(\"Initialize all Variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#init parameters\n",
    "d_loss = 1.0\n",
    "lr_ = lr\n",
    "outputs_series = list()\n",
    "average_loss_series = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9460c3acf34244439a756ace027bb4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Average Loss: 3.21230629512 Learning Rate: 2.5\n",
      "Epoch: 100 Average Loss: 3.14958832605 Learning Rate: 2.5\n",
      "Epoch: 200 Average Loss: 3.11597924005 Learning Rate: 2.5\n",
      "Epoch: 300 Average Loss: 3.10926295689 Learning Rate: 2.5\n",
      "Epoch: 400 Average Loss: 3.09750381424 Learning Rate: 2.5\n",
      "Epoch: 500 Average Loss: 3.09154354277 Learning Rate: 2.5\n",
      "Epoch: 600 Average Loss: 3.08472666286 Learning Rate: 2.5\n",
      "Epoch: 700 Average Loss: 3.07533471925 Learning Rate: 2.5\n",
      "Epoch: 800 Average Loss: 3.07099341211 Learning Rate: 2.5\n",
      "Epoch: 900 Average Loss: 3.06813503901 Learning Rate: 2.5\n",
      "Epoch: 1000 Average Loss: 3.06781842595 Learning Rate: 0.8333333333333334\n",
      "Epoch: 1100 Average Loss: 3.06244245484 Learning Rate: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# train SCRN\n",
    "#lr_ = 0.1\n",
    "#per_epoch = 100\n",
    "#global_epoch = tf.Variable(0)\n",
    "#Epochs = 30000\n",
    "steps = len(train_data_stepy)\n",
    "pbar = tqdm(range(Epochs))\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    feed_dict = {}\n",
    "    feed_dict[learning_rate] = lr_\n",
    "    \n",
    "    average_loss = list()\n",
    "    \n",
    "    for step in range(steps):\n",
    "\n",
    "        #initialize the output\n",
    "        if step == 0:\n",
    "            \n",
    "            if epoch == 0:\n",
    "                h_pass = np.zeros([batch_size, h_size], dtype=np.float32)\n",
    "                s_pass = np.zeros([batch_size, s_size], dtype=np.float32)\n",
    "            else:\n",
    "                h_pass = np.zeros([batch_size, h_size], dtype=np.float32)\n",
    "                h_pass[1:batch_size,:] = np.asarray(h_after_[0:batch_size-1,:])\n",
    "                s_pass = np.zeros([batch_size, s_size], dtype=np.float32)\n",
    "                s_pass[1:batch_size,:] = np.asarray(s_after_[0:batch_size-1,:])\n",
    "                \n",
    "            h_pass = np.zeros([batch_size, h_size], dtype=np.float32)\n",
    "            s_pass = np.zeros([batch_size, s_size], dtype=np.float32)\n",
    "        else:\n",
    "            h_pass = h_after_\n",
    "            s_pass = s_after_\n",
    "        #feed_dict={h_first: h_pass, s_first:s_pass}\n",
    "        feed_dict[h_first] = h_pass\n",
    "        feed_dict[s_first] = s_pass\n",
    "\n",
    "#11 steps, 32 batches, 10 bp, 5 fw\n",
    "        #trains x\n",
    "        for i in range(num_bp+num_fw):\n",
    "            if i <num_bp-1:\n",
    "                batch_x_i = train_data_stepx[step][:,i,0].astype(int)\n",
    "            else:\n",
    "                batch_x_i = train_data_stepx[step][:,num_bp-1,i-num_bp].astype(int)\n",
    "            batch_x_i.reshape([batch_size,1])\n",
    "            batch_x_i_onehot = np.zeros((len(batch_x_i), input_size))\n",
    "            batch_x_i_onehot[range(len(batch_x_i)), batch_x_i] = 1\n",
    "            feed_dict[trainX[i]] = batch_x_i_onehot\n",
    "            \n",
    "        for i in range(num_bp):\n",
    "            batch_y_i = train_data_stepy[step][:,i].astype(int)\n",
    "            batch_y_i.reshape([batch_size,1])\n",
    "            batch_y_i_onehot = np.zeros((len(batch_y_i), output_size))\n",
    "            batch_y_i_onehot[range(len(batch_y_i)), batch_y_i]=1\n",
    "            feed_dict[trainY[i]] = batch_y_i_onehot\n",
    "\n",
    "        #feed_dict[learning_rate] = 0.1\n",
    "\n",
    "        h_after_, s_after_, l ,outputs_, opt_ = sess.run([h_after, s_after, loss, outputs, opt], feed_dict=feed_dict)\n",
    "        outputs_series.append(outputs_)\n",
    "        average_loss.append(l)\n",
    "        \n",
    "    ave_loss = sum(average_loss)/float(len(average_loss))\n",
    "        \n",
    "    average_loss_series.append(ave_loss)\n",
    "        \n",
    "    pbar.update(1)\n",
    "\n",
    "    #average_loss_series.append(ave_loss)\n",
    "    global_epoch += 1\n",
    "    if epoch%per_epoch == 0:\n",
    "        \n",
    "        #average_loss_series.append(ave_loss)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            d_loss = average_loss_series[0]\n",
    "            ave_loss_pre = ave_loss\n",
    "        else:\n",
    "            d_loss = ave_loss_pre - ave_loss\n",
    "            ave_loss_pre = ave_loss\n",
    "        \n",
    "        \n",
    "        if np.abs(d_loss)<1.0e-3:\n",
    "            if lr_ > 0.01:\n",
    "                lr_ = lr_/3.0\n",
    "            if lr_ < 0.01:\n",
    "                lr_ = 0.01\n",
    "        \n",
    "        \n",
    "        print('Epoch: '+str(epoch)+ ' Average Loss: ' +str(ave_loss) + ' Learning Rate: ' +str(lr_))\n",
    "    #if ave_loss<0.8:\n",
    "    #    break\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2aea5d3415b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#save training process:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msave_train_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model saved in file: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msave_train_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'saver' is not defined"
     ]
    }
   ],
   "source": [
    "#save training process:\n",
    "save_train_path = saver.save(sess, cwd+\"/\"+train_filename)\n",
    "print(\"Model saved in file: %s\" % save_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot tuning curve\n",
    "f1 = plt.figure()\n",
    "plt.plot(average_loss_series)\n",
    "plt.show(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict and evaluate\n",
    "def eval_(data_x=eval_data_x, data_y=eval_data_y, num_fw=num_fw):\n",
    "    evalX=[]\n",
    "    y_ = []\n",
    "    char_= list()\n",
    "    for ii in range(num_fw):\n",
    "        evalX.append(tf.placeholder(tf.float32, shape=(1, input_size)))\n",
    "    \n",
    "    s_first_eval = tf.placeholder(tf.float32, shape=(1, s_size), name='1st_s_eval')\n",
    "    h_first_eval = tf.placeholder(tf.float32, shape=(1, h_size), name='1st_h_eval')\n",
    "    \n",
    "    for j in range(num_fw):\n",
    "        if j<num_fw-1:\n",
    "            s_after_eval, h_after_eval, _ = SCRN(evalX[j], s_first_eval, h_first_eval)\n",
    "        else:\n",
    "            s_after_eval, h_after_eval,y_after_eval = SCRN(evalX[j], s_pre_eval, h_pre_eval)\n",
    "        s_pre_eval = s_after_eval\n",
    "        h_pre_eval = h_after_eval\n",
    "    outputs_eval = tf.argmax(y_after_eval)\n",
    "    \n",
    "    acc_eva = 0\n",
    "    for step in range(len(data_y)):\n",
    "        #feed dictory\n",
    "        if step == 0:\n",
    "            h_pass_eval = np.zeros([1, h_size], dtype=np.float32)\n",
    "            s_pass_eval = np.zeros([1, s_size], dtype=np.float32)\n",
    "        else:\n",
    "            h_pass_eval = h_after_eval_\n",
    "            s_pass_eval = s_after_eval_\n",
    "        feed_dict_eval={h_first_eval: h_pass_eval, s_first_eval:s_pass_eval}\n",
    "        for ii in range(num_fw):\n",
    "            data_x_i = data_x[step][ii].reshape([1])\n",
    "            data_x_i_onehot = np.zeros([1,input_size])\n",
    "            #print(step, ii, data_x_i)\n",
    "            data_x_i_onehot[range(1),data_x_i] = 1\n",
    "            feed_dict_eval[evalX[ii]] = data_x_i_onehot\n",
    "            \n",
    "        h_after_eval_, s_after_eval_, outputs_eval_, y_after_eval_= sess.run(\n",
    "            [h_after_eval, s_after_eval, outputs_eval, y_after_eval], feed_dict=feed_dict_eval)\n",
    "        \n",
    "        y_.append(chr(np.argmax(y_after_eval_)+ord('a')))\n",
    "        \n",
    "        if np.argmax(y_after_eval_) == data_y[step].reshape([]):\n",
    "            acc_eva += 1\n",
    "            #print(y_[-1],chr(data_y[step]+ord('a')))\n",
    "            char_.append(y_[-1])\n",
    "    \n",
    "    print('List of successfully recalled characters: %s' %(list(set(char_))))\n",
    "    acc = acc_eva/len(data_y)\n",
    "    print('Accuracy: %f'%(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc =eval_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results(generate a paper? recall the memory?)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "340fa0b33c1d4059b9564af62e9f4c4f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5538f6299cdb417e95432e78f3698ff7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "588c33131f0744a18c92f31dc2ac7f05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6d66af9294cd407cbd7924b00645bd73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6dced2ef873c45be9383e658d1205c54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b2f77b5624b341b0b7b5f5adfdc92b12",
        "IPY_MODEL_b99e6cf99ead4c9f8a80d5ed0b8cc68d"
       ],
       "layout": "IPY_MODEL_340fa0b33c1d4059b9564af62e9f4c4f"
      }
     },
     "9d1664971b9845fc90fa862fe168d78e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9da632b6776c4295b7a26c5bdf463233": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ab885703ab5445b59440e8d906e40e62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_588c33131f0744a18c92f31dc2ac7f05",
       "style": "IPY_MODEL_9da632b6776c4295b7a26c5bdf463233",
       "value": "  5% 302/6000 [21:43&lt;7:16:45,  4.60s/it]"
      }
     },
     "b10c731b2edd4ea3b6e297e3ba00c902": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b2f77b5624b341b0b7b5f5adfdc92b12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "IntProgressModel",
      "state": {
       "layout": "IPY_MODEL_d5c417d354cb4c8ea456cbb6126f41e9",
       "max": 6000,
       "style": "IPY_MODEL_d1734b310e914babb6e2677d062638ac",
       "value": 104
      }
     },
     "b83cc4a9c7f5485ea4f6f6744f224bd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "IntProgressModel",
      "state": {
       "layout": "IPY_MODEL_6d66af9294cd407cbd7924b00645bd73",
       "max": 6000,
       "style": "IPY_MODEL_b10c731b2edd4ea3b6e297e3ba00c902",
       "value": 302
      }
     },
     "b99e6cf99ead4c9f8a80d5ed0b8cc68d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9d1664971b9845fc90fa862fe168d78e",
       "style": "IPY_MODEL_5538f6299cdb417e95432e78f3698ff7",
       "value": "  2% 104/6000 [09:47&lt;8:55:19,  5.45s/it]"
      }
     },
     "c646f172c1eb4b2cb874ada5cc06cb53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b83cc4a9c7f5485ea4f6f6744f224bd8",
        "IPY_MODEL_ab885703ab5445b59440e8d906e40e62"
       ],
       "layout": "IPY_MODEL_f46ec122d47d4a8e98fe901ce45f6127"
      }
     },
     "d1734b310e914babb6e2677d062638ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d5c417d354cb4c8ea456cbb6126f41e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f46ec122d47d4a8e98fe901ce45f6127": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
