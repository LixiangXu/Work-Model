{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "Strunturally constrained recurrent network(SCRN)\n",
    "Goal is to predict the next token in the sequence given its past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import package\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time as time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data import parameter\n",
    "#filename_input = \"words.txt\"\n",
    "filename_input = \"words_total.txt\"\n",
    "eval_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NN parameter\n",
    "input_size = 26 # one-hot input of characters\n",
    "h_size = 40#200 # size of hidden layer(fast)\n",
    "output_size = 26 # output size, one-hot output of character\n",
    "num_fw = 5 # forward steps\n",
    "num_bp = 10 # BPTT steps\n",
    "lr = 2.5 # initial learning rate\n",
    "batch_size = 32\n",
    "Epochs = 6000 # number of epochs\n",
    "per_epoch = 100 # epochs to display loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import file: words_total.txt\n"
     ]
    }
   ],
   "source": [
    "# import data from dataset, articles or reviews\n",
    "with open(filename_input) as file_input:\n",
    "    data_str = file_input.read()\n",
    "print('Import file: '+filename_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = int((len(data_str)-num_fw)/num_bp/batch_size)\n",
    "data_mat = np.asarray([ord(char)-ord('a') for char in data_str])\n",
    "\n",
    "train_data_len = int((steps-eval_steps)*num_bp*batch_size)\n",
    "eval_data_len = int(eval_steps*num_bp*batch_size)\n",
    "\n",
    "train_data_mat = np.zeros([train_data_len, num_fw])\n",
    "train_data_y = np.zeros([train_data_len])\n",
    "eval_data_x = np.zeros([eval_data_len, num_fw])\n",
    "eval_data_y = np.zeros([eval_data_len])\n",
    "train_data_x = list()\n",
    "train_data_y = list()\n",
    "eval_data_x = list()\n",
    "eval_data_y = list()\n",
    "\n",
    "for ii in range(train_data_len):\n",
    "    train_data_x.append(data_mat[ii:ii+num_fw])\n",
    "    train_data_y.append(data_mat[ii+num_fw])\n",
    "for ii in range(eval_data_len):\n",
    "    jj = ii + train_data_len\n",
    "    eval_data_x.append(data_mat[jj:jj+num_fw])\n",
    "    eval_data_y.append(data_mat[jj+num_fw])\n",
    "\n",
    "train_data_batchx = list()\n",
    "train_data_batchy = list()\n",
    "per_batch = int((steps-eval_steps)*num_bp)\n",
    "for i in range(batch_size):\n",
    "    train_data_batchx.append(train_data_x[i*per_batch: (i+1)*per_batch])\n",
    "    train_data_batchy.append(train_data_y[i*per_batch: (i+1)*per_batch])\n",
    "\n",
    "train_data_batchx = np.asarray(train_data_batchx)\n",
    "train_data_batchy = np.asarray(train_data_batchy)\n",
    "train_data_stepx = list()\n",
    "train_data_stepy = list()\n",
    "for i in range(int(steps-eval_steps)):\n",
    "    train_data_stepx.append(train_data_batchx[:, i*num_bp:(i+1)*num_bp, :])\n",
    "    train_data_stepy.append(train_data_batchy[:, i*num_bp:(i+1)*num_bp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(11 steps, 32 batches, 10 bps, 5 fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define structure of SCRN\n",
    "trainX = list()\n",
    "trainY = list()\n",
    "for i in range(num_fw + num_bp):\n",
    "    trainX.append(tf.placeholder(tf.float32, shape=(batch_size, input_size)))\n",
    "for i in range(num_bp):\n",
    "    trainY.append(tf.placeholder(tf.float32, shape=(batch_size, output_size)))\n",
    "    \n",
    "h_first = tf.placeholder(tf.float32, shape=(batch_size, h_size), name='1st_h')\n",
    "\n",
    "A = tf.Variable(tf.truncated_normal([input_size, h_size], -0.1,0.1), name='A')\n",
    "R = tf.Variable(tf.truncated_normal([h_size, h_size], -0.1,0.1), name='R')\n",
    "\n",
    "U = tf.Variable(tf.truncated_normal([h_size, output_size], -0.1,0.1), name='U')\n",
    "\n",
    "\n",
    "def SCRN(x, h_pre):\n",
    "    h_after = tf.sigmoid(tf.matmul(x, A)+tf.matmul(h_pre,R)) #h_t = sigmoid(Ps_t+Ax_t+Rh_t-1)\n",
    "    y_after = tf.nn.softmax(tf.matmul(h_after, U)) #y_t = f(Uh_t + Vs_t)\n",
    "    \n",
    "    return h_after, y_after\n",
    "\n",
    "\n",
    "# forward\n",
    "for i in range(num_bp):\n",
    "    j = 0\n",
    "    if i == 0:\n",
    "        outputs = list()\n",
    "        h_pre = h_first\n",
    "    \n",
    "    for j in range(num_fw):\n",
    "        if j<num_fw-1:\n",
    "            h_after, _ = SCRN(trainX[i+j], h_pre)\n",
    "        else:\n",
    "            h_after,y_after = SCRN(trainX[i+j], h_pre)\n",
    "        \n",
    "        \"\"\"\n",
    "        #dropout\n",
    "        h_after = tf.nn.dropout(h_after, keep_prob=0.9)\n",
    "        \"\"\"\n",
    "        \n",
    "        h_pre = h_after\n",
    "    \n",
    "    \n",
    "    outputs.append(y_after)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train opt\n",
    "\n",
    "#log likelihood loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.concat(outputs,0),labels=tf.concat(trainY,0)))\n",
    "#loss = tf.losses.mean_squared_error(labels=tf.concat(trainY,0), predictions = tf.concat(outputs,0))\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "#optimizer\n",
    "global_epoch = tf.Variable(0)\n",
    "#learning_rate = tf.train.exponential_decay(\n",
    "#    learning_rate=, global_step=global_epoch, decay_steps=5000, decay_rate=0.9, staircase=True)\n",
    "#learning_rate = lr\n",
    "\n",
    "#optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.5, use_nesterov=True)\n",
    "\n",
    "\n",
    "gradients,var=zip(*optimizer.compute_gradients(loss))\n",
    "gradients_clipped, _ = tf.clip_by_global_norm(gradients, 0.1)\n",
    "opt=optimizer.apply_gradients(zip(gradients_clipped,var),global_step=global_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add init op to the graph\n",
    "init = tf.global_variables_initializer()\n",
    "steps = len(train_data_stepy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#init parameters\n",
    "d_loss = 1.0\n",
    "lr_ = lr\n",
    "outputs_series = list()\n",
    "average_loss_series = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4983223314d342f2bc70a02465bc55a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Average Loss: 3.07893323898 Learning Rate: 2.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lixiang\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\lixiang\\src\\tqdm\\tqdm\\_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\Lixiang\\Anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 Average Loss: 3.08237791061 Learning Rate: 1.6666666666666667\n",
      "Epoch: 200 Average Loss: 3.07296514511 Learning Rate: 1.6666666666666667\n",
      "Epoch: 300 Average Loss: 3.07159471512 Learning Rate: 1.6666666666666667\n",
      "Epoch: 400 Average Loss: 3.07316350937 Learning Rate: 1.1111111111111112\n",
      "Epoch: 500 Average Loss: 3.0693666935 Learning Rate: 1.1111111111111112\n",
      "Epoch: 600 Average Loss: 3.06880712509 Learning Rate: 0.7407407407407408\n"
     ]
    }
   ],
   "source": [
    "# train SCRN\n",
    "#lr_ = 1.0\n",
    "#per_epoch = 100\n",
    "#global_epoch = tf.Variable(0)\n",
    "#Epochs = 30000\n",
    "pbar = tqdm(range(Epochs))\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    feed_dict = {}\n",
    "    feed_dict[learning_rate] = lr_\n",
    "    \n",
    "    for step in range(steps):\n",
    "\n",
    "        average_loss = list()\n",
    "        #initialize the output\n",
    "        if step == 0:\n",
    "            \n",
    "            if epoch == 0:\n",
    "                h_pass = np.zeros([batch_size, h_size], dtype=np.float32)\n",
    "            else:\n",
    "                h_pass = np.zeros([batch_size, h_size], dtype=np.float32)\n",
    "                h_pass[1:batch_size,:] = np.asarray(h_after_[0:batch_size-1,:])\n",
    "                \n",
    "            h_pass = np.zeros([batch_size, h_size], dtype=np.float32)\n",
    "        else:\n",
    "            h_pass = h_after_\n",
    "            \n",
    "        #feed directory\n",
    "        feed_dict[h_first] = h_pass\n",
    "\n",
    "#11 steps, 32 batches, 10 bp, 5 fw\n",
    "        #trains x\n",
    "        for i in range(num_bp+num_fw):\n",
    "            if i <num_bp-1:\n",
    "                batch_x_i = train_data_stepx[step][:,i,0].astype(int)\n",
    "            else:\n",
    "                batch_x_i = train_data_stepx[step][:,num_bp-1,i-num_bp].astype(int)\n",
    "            batch_x_i.reshape([batch_size,1])\n",
    "            batch_x_i_onehot = np.zeros((len(batch_x_i), input_size))\n",
    "            batch_x_i_onehot[range(len(batch_x_i)), batch_x_i] = 1\n",
    "            feed_dict[trainX[i]] = batch_x_i_onehot\n",
    "            \n",
    "        for i in range(num_bp):\n",
    "            batch_y_i = train_data_stepy[step][:,i].astype(int)\n",
    "            batch_y_i.reshape([batch_size,1])\n",
    "            batch_y_i_onehot = np.zeros((len(batch_y_i), output_size))\n",
    "            batch_y_i_onehot[range(len(batch_y_i)), batch_y_i]=1\n",
    "            feed_dict[trainY[i]] = batch_y_i_onehot\n",
    "\n",
    "        #feed_dict[learning_rate] = 0.1\n",
    "\n",
    "        h_after_, l ,outputs_, opt_ = sess.run([h_after, loss, outputs, opt], feed_dict=feed_dict)\n",
    "        outputs_series.append(outputs_)\n",
    "        average_loss.append(l)\n",
    "        ave_loss = sum(average_loss)/float(len(average_loss))\n",
    "\n",
    "        \n",
    "    pbar.update(1)\n",
    "\n",
    "    #average_loss_series.append(ave_loss)\n",
    "    global_epoch += 1\n",
    "    if epoch%per_epoch == 0:\n",
    "        average_loss_series.append(ave_loss)\n",
    "        \n",
    "        if len(average_loss_series) == 1:\n",
    "            d_loss = average_loss_series[0]\n",
    "            ave_loss_pre = ave_loss\n",
    "        else:\n",
    "            d_loss = ave_loss_pre - ave_loss\n",
    "            ave_loss_pre = ave_loss\n",
    "        \n",
    "        \n",
    "        if np.abs(d_loss)<1.0e-3:\n",
    "            lr_ = lr_/1.5\n",
    "        \n",
    "        \n",
    "        print('Epoch: '+str(epoch)+ ' Average Loss: ' +str(ave_loss) + ' Learning Rate: ' +str(lr_))\n",
    "    #if ave_loss<0.8:\n",
    "    #    break\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot tuning curve\n",
    "f1 = plt.figure()\n",
    "plt.plot(average_loss_series)\n",
    "plt.show(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict and evaluate\n",
    "def eval_(data_x=eval_data_x, data_y=eval_data_y, num_fw=num_fw):\n",
    "    evalX=[]\n",
    "    y_ = []\n",
    "    char_= list()\n",
    "    for ii in range(num_fw):\n",
    "        evalX.append(tf.placeholder(tf.float32, shape=(1, input_size)))\n",
    "    \n",
    "    h_first_eval = tf.placeholder(tf.float32, shape=(1, h_size), name='1st_h_eval')\n",
    "    \n",
    "    for j in range(num_fw):\n",
    "        if j<num_fw-1:\n",
    "            h_after_eval, _ = SCRN(evalX[j], h_first_eval)\n",
    "        else:\n",
    "            h_after_eval,y_after_eval = SCRN(evalX[j], h_pre_eval)\n",
    "        h_pre_eval = h_after_eval\n",
    "    outputs_eval = tf.argmax(y_after_eval)\n",
    "    \n",
    "    acc_eva = 0\n",
    "    for step in range(len(data_y)):\n",
    "        #feed dictory\n",
    "        if step == 0:\n",
    "            h_pass_eval = np.zeros([1, h_size], dtype=np.float32)\n",
    "        else:\n",
    "            h_pass_eval = h_after_eval_\n",
    "        feed_dict_eval={h_first_eval: h_pass_eval}\n",
    "        for ii in range(num_fw):\n",
    "            data_x_i = data_x[step][ii].reshape([1])\n",
    "            data_x_i_onehot = np.zeros([1,input_size])\n",
    "            data_x_i_onehot[range(1),data_x_i] = 1\n",
    "            feed_dict_eval[evalX[ii]] = data_x_i_onehot\n",
    "            \n",
    "        h_after_eval_, outputs_eval_, y_after_eval_= sess.run(\n",
    "            [h_after_eval, outputs_eval, y_after_eval], feed_dict=feed_dict_eval)\n",
    "        \n",
    "        y_.append(chr(np.argmax(y_after_eval_)+ord('a')))\n",
    "        \n",
    "        if np.argmax(y_after_eval_) == data_y[step].reshape([]):\n",
    "            acc_eva += 1\n",
    "            #print(y_[-1],chr(data_y[step]+ord('a')))\n",
    "            char_.append(y_[-1])\n",
    "    \n",
    "    print('List of successfully recalled characters: %s' %(list(set(char_))))\n",
    "    acc = acc_eva/len(data_y)\n",
    "    print('Accuracy: %f'%(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc =eval_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results(generate a paper? recall the memory?)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
